{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2b26c517",
   "metadata": {},
   "source": [
    "# Project Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acace69a",
   "metadata": {},
   "source": [
    "For this project we'll use the Cornell University Movie Review polarity dataset v2.0 obtained from http://www.cs.cornell.edu/people/pabo/movie-review-data/\n",
    "\n",
    "\n",
    "We will try to predict whether a review is considered negative or positive"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3dbf28c",
   "metadata": {},
   "source": [
    "# Importing Basic Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ccb7298b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# These are the libraries I typically use in my analysis so I find it easier to import them all at once\n",
    "# If I need more libraries I will import them as needed\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "plt.style.use('fivethirtyeight')\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "698311a1",
   "metadata": {},
   "source": [
    "# Importing the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7cace516",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Our dataset is moviereviews.tsv, where the tsv stands for \"tab separated variables\"\n",
    "# Hence in order to import the file correctly we need to add delimiter = \"\\t\"\n",
    "# We will name the dataframe \"movies\"\n",
    "\n",
    "movies =  pd.read_csv('moviereviews.tsv', delimiter = '\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2148653c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>review</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>neg</td>\n",
       "      <td>how do films like mouse hunt get into theatres...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>neg</td>\n",
       "      <td>some talented actresses are blessed with a dem...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>pos</td>\n",
       "      <td>this has been an extraordinary year for austra...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>pos</td>\n",
       "      <td>according to hollywood movies made in last few...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>neg</td>\n",
       "      <td>my first press screening of 1998 and already i...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  label                                             review\n",
       "0   neg  how do films like mouse hunt get into theatres...\n",
       "1   neg  some talented actresses are blessed with a dem...\n",
       "2   pos  this has been an extraordinary year for austra...\n",
       "3   pos  according to hollywood movies made in last few...\n",
       "4   neg  my first press screening of 1998 and already i..."
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "movies.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a1aba973",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2000, 2)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# There are 2000 movie reviews in our dataset\n",
    "\n",
    "movies.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b1be3e03",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "label      0\n",
       "review    35\n",
       "dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We do not have any missing values in the label column\n",
    "# We do however have 35 missing values in the review column\n",
    "\n",
    "movies.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "96978bf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we will drop the missing values\n",
    "\n",
    "movies.dropna(inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a43f4686",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "label     0\n",
       "review    0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Just confirming that we no longer have any null values\n",
    "\n",
    "movies.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "57b1e362",
   "metadata": {},
   "outputs": [],
   "source": [
    "# As an extra check, we will create our own way of iterating through the dataset to look for empty strings\n",
    "\n",
    "# So here we will initialize an empty list\n",
    "blanks = []\n",
    "\n",
    "# We will get a tuple with the index location, label value, and the review text itself\n",
    "# i = index, lb = label value, rv = review\n",
    "for i,lb,rv in movies.itertuples():\n",
    "    # isspace() checks if there is just a whitespace and no other values\n",
    "    # \"If the review is whitespace\"\n",
    "    if rv.isspace():\n",
    "        # Add the index position to our empty list me initiated above\n",
    "        blanks.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0911fb2d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[57,\n",
       " 71,\n",
       " 147,\n",
       " 151,\n",
       " 283,\n",
       " 307,\n",
       " 313,\n",
       " 323,\n",
       " 343,\n",
       " 351,\n",
       " 427,\n",
       " 501,\n",
       " 633,\n",
       " 675,\n",
       " 815,\n",
       " 851,\n",
       " 977,\n",
       " 1079,\n",
       " 1299,\n",
       " 1455,\n",
       " 1493,\n",
       " 1525,\n",
       " 1531,\n",
       " 1763,\n",
       " 1851,\n",
       " 1905,\n",
       " 1993]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Here are all the indices where the review is just empty space\n",
    "\n",
    "blanks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d6c61a99",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will remove the rows with empty reviews by passing in our blanks list to our dataframe\n",
    "\n",
    "movies.drop(blanks, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0ad23b7c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "neg    969\n",
       "pos    969\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Looks like we have a perfectly symmetrical breakdown of positive and negative reviews after the cleaning\n",
    "\n",
    "movies['label'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "546eda90",
   "metadata": {},
   "source": [
    "# Creating Our Classification Model Part One"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "38550e08",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting up our X and y values\n",
    "\n",
    "X = movies['review']\n",
    "y = movies['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9a8a2cca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting the data between a training and test set\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d77fe2ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's build a pipeline to vectorize the data\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e5446280",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The first model we will train will be a Naive Bayes classifier\n",
    "\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "text_clf_nb = Pipeline([('tfidf', TfidfVectorizer()),\n",
    "                     ('clf', MultinomialNB())])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6907bab8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('tfidf', TfidfVectorizer()), ('clf', MultinomialNB())])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fitting the data to the NB Classifier\n",
    "\n",
    "text_clf_nb.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "30d45ad5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Form a prediction set\n",
    "\n",
    "y_pred = text_clf_nb.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9d90edce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[174  14]\n",
      " [ 65 135]]\n"
     ]
    }
   ],
   "source": [
    "# Report the confusion matrix\n",
    "\n",
    "from sklearn import metrics\n",
    "print(metrics.confusion_matrix(y_test,y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "58c49e13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         neg       0.73      0.93      0.81       188\n",
      "         pos       0.91      0.68      0.77       200\n",
      "\n",
      "    accuracy                           0.80       388\n",
      "   macro avg       0.82      0.80      0.79       388\n",
      "weighted avg       0.82      0.80      0.79       388\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Print a classification report\n",
    "\n",
    "print(metrics.classification_report(y_test,y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c67d9acc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7963917525773195\n"
     ]
    }
   ],
   "source": [
    "# Print the overall accuracy\n",
    "\n",
    "print(metrics.accuracy_score(y_test,y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "97b95e7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we will create our own personal review\n",
    "# We will then test our model on the review and see what it predicts\n",
    "\n",
    "myreview = \"A movie I really wanted to love was terrible. \\\n",
    "I'm sure the producers had the best intentions, but the execution was lacking.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e0e9caab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['neg']\n"
     ]
    }
   ],
   "source": [
    "# Here we will run our model on a new review\n",
    "# Be sure to put \"myreview\" inside square brackets\n",
    "# The review is considered negative by our model\n",
    "print(text_clf_nb.predict([myreview])) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ad164c8",
   "metadata": {},
   "source": [
    "# Creating Our Classification Model Part Two"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "212e8132",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Our second model will be a Support Vector Machine Classifier\n",
    "\n",
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "text_clf_lsvc = Pipeline([('tfidf', TfidfVectorizer()),\n",
    "                     ('clf', LinearSVC())])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "3a5b292e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('tfidf', TfidfVectorizer()), ('clf', LinearSVC())])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fitting the data to the SVC Classifier\n",
    "\n",
    "text_clf_lsvc.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "4236eaf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Form a prediction set\n",
    "\n",
    "y_pred = text_clf_lsvc.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "909fa68a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[162  26]\n",
      " [ 26 174]]\n"
     ]
    }
   ],
   "source": [
    "# Report the confusion matrix\n",
    "\n",
    "from sklearn import metrics\n",
    "print(metrics.confusion_matrix(y_test,y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "052d2e38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         neg       0.86      0.86      0.86       188\n",
      "         pos       0.87      0.87      0.87       200\n",
      "\n",
      "    accuracy                           0.87       388\n",
      "   macro avg       0.87      0.87      0.87       388\n",
      "weighted avg       0.87      0.87      0.87       388\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Print a classification report\n",
    "\n",
    "print(metrics.classification_report(y_test,y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "588a0eb6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.865979381443299\n"
     ]
    }
   ],
   "source": [
    "# Print the overall accuracy\n",
    "\n",
    "print(metrics.accuracy_score(y_test,y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "7b62f411",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['neg']\n"
     ]
    }
   ],
   "source": [
    "# Here we will run our model on a new review\n",
    "# Be sure to put \"myreview\" inside square brackets\n",
    "# The review is considered negative by our model\n",
    "\n",
    "print(text_clf_lsvc.predict([myreview]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0dafca0",
   "metadata": {},
   "source": [
    "# Adding Stopwords to CountVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89064808",
   "metadata": {},
   "source": [
    "By default, CountVectorizer and TfidfVectorizer do not filter stopwords. However, they offer some optional settings, including passing in your own stopword list."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12d298c9",
   "metadata": {},
   "source": [
    "The CountVectorizer class accepts the following arguments:\n",
    "\n",
    "CountVectorizer(input='content', encoding='utf-8', decode_error='strict', strip_accents=None, lowercase=True, preprocessor=None, tokenizer=None, stop_words=None, token_pattern='(?u)\\b\\w\\w+\\b', ngram_range=(1, 1), analyzer='word', max_df=1.0, min_df=1, max_features=None, vocabulary=None, binary=False, dtype=<class 'numpy.int64'>)\n",
    "\n",
    "TfidVectorizer supports the same arguments and more. Under stop_words we have the following options:\n",
    "\n",
    "stop_words : string {'english'}, list, or None (default)\n",
    "\n",
    "That is, we can run TfidVectorizer(stop_words='english') to accept scikit-learn's built-in list,\n",
    "or TfidVectorizer(stop_words=[a, and, the]) to filter these three words. In practice we would assign our list to a variable and pass that in instead."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5b5752b",
   "metadata": {},
   "source": [
    "Scikit-learn's built-in list contains 318 stopwords:\n",
    "\n",
    "from sklearn.feature_extraction import text\n",
    "print(text.ENGLISH_STOP_WORDS)\n",
    "['a', 'about', 'above', 'across', 'after', 'afterwards', 'again', 'against', 'all', 'almost', 'alone', 'along', 'already', 'also', 'although', 'always', 'am', 'among', 'amongst', 'amoungst', 'amount', 'an', 'and', 'another', 'any', 'anyhow', 'anyone', 'anything', 'anyway', 'anywhere', 'are', 'around', 'as', 'at', 'back', 'be', 'became', 'because', 'become', 'becomes', 'becoming', 'been', 'before', 'beforehand', 'behind', 'being', 'below', 'beside', 'besides', 'between', 'beyond', 'bill', 'both', 'bottom', 'but', 'by', 'call', 'can', 'cannot', 'cant', 'co', 'con', 'could', 'couldnt', 'cry', 'de', 'describe', 'detail', 'do', 'done', 'down', 'due', 'during', 'each', 'eg', 'eight', 'either', 'eleven', 'else', 'elsewhere', 'empty', 'enough', 'etc', 'even', 'ever', 'every', 'everyone', 'everything', 'everywhere', 'except', 'few', 'fifteen', 'fifty', 'fill', 'find', 'fire', 'first', 'five', 'for', 'former', 'formerly', 'forty', 'found', 'four', 'from', 'front', 'full', 'further', 'get', 'give', 'go', 'had', 'has', 'hasnt', 'have', 'he', 'hence', 'her', 'here', 'hereafter', 'hereby', 'herein', 'hereupon', 'hers', 'herself', 'him', 'himself', 'his', 'how', 'however', 'hundred', 'i', 'ie', 'if', 'in', 'inc', 'indeed', 'interest', 'into', 'is', 'it', 'its', 'itself', 'keep', 'last', 'latter', 'latterly', 'least', 'less', 'ltd', 'made', 'many', 'may', 'me', 'meanwhile', 'might', 'mill', 'mine', 'more', 'moreover', 'most', 'mostly', 'move', 'much', 'must', 'my', 'myself', 'name', 'namely', 'neither', 'never', 'nevertheless', 'next', 'nine', 'no', 'nobody', 'none', 'noone', 'nor', 'not', 'nothing', 'now', 'nowhere', 'of', 'off', 'often', 'on', 'once', 'one', 'only', 'onto', 'or', 'other', 'others', 'otherwise', 'our', 'ours', 'ourselves', 'out', 'over', 'own', 'part', 'per', 'perhaps', 'please', 'put', 'rather', 're', 'same', 'see', 'seem', 'seemed', 'seeming', 'seems', 'serious', 'several', 'she', 'should', 'show', 'side', 'since', 'sincere', 'six', 'sixty', 'so', 'some', 'somehow', 'someone', 'something', 'sometime', 'sometimes', 'somewhere', 'still', 'such', 'system', 'take', 'ten', 'than', 'that', 'the', 'their', 'them', 'themselves', 'then', 'thence', 'there', 'thereafter', 'thereby', 'therefore', 'therein', 'thereupon', 'these', 'they', 'thick', 'thin', 'third', 'this', 'those', 'though', 'three', 'through', 'throughout', 'thru', 'thus', 'to', 'together', 'too', 'top', 'toward', 'towards', 'twelve', 'twenty', 'two', 'un', 'under', 'until', 'up', 'upon', 'us', 'very', 'via', 'was', 'we', 'well', 'were', 'what', 'whatever', 'when', 'whence', 'whenever', 'where', 'whereafter', 'whereas', 'whereby', 'wherein', 'whereupon', 'wherever', 'whether', 'which', 'while', 'whither', 'who', 'whoever', 'whole', 'whom', 'whose', 'why', 'will', 'with', 'within', 'without', 'would', 'yet', 'you', 'your', 'yours', 'yourself', 'yourselves']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "53187bcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's trim the list to just 60 words:\n",
    "\n",
    "stopwords = ['a', 'about', 'an', 'and', 'are', 'as', 'at', 'be', 'been', 'but', 'by', 'can', \\\n",
    "             'even', 'ever', 'for', 'from', 'get', 'had', 'has', 'have', 'he', 'her', 'hers', 'his', \\\n",
    "             'how', 'i', 'if', 'in', 'into', 'is', 'it', 'its', 'just', 'me', 'my', 'of', 'on', 'or', \\\n",
    "             'see', 'seen', 'she', 'so', 'than', 'that', 'the', 'their', 'there', 'they', 'this', \\\n",
    "             'to', 'was', 'we', 'were', 'what', 'when', 'which', 'who', 'will', 'with', 'you']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "352452af",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('tfidf',\n",
       "                 TfidfVectorizer(stop_words=['a', 'about', 'an', 'and', 'are',\n",
       "                                             'as', 'at', 'be', 'been', 'but',\n",
       "                                             'by', 'can', 'even', 'ever', 'for',\n",
       "                                             'from', 'get', 'had', 'has',\n",
       "                                             'have', 'he', 'her', 'hers', 'his',\n",
       "                                             'how', 'i', 'if', 'in', 'into',\n",
       "                                             'is', ...])),\n",
       "                ('clf', LinearSVC())])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Here we will add Stopwords to the Linear SVC Pipeline:\n",
    "\n",
    "text_clf_lsvc2 = Pipeline([('tfidf', TfidfVectorizer(stop_words=stopwords)),\n",
    "                     ('clf', LinearSVC())])\n",
    "\n",
    "\n",
    "text_clf_lsvc2.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "75bffbf5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[160  28]\n",
      " [ 33 167]]\n"
     ]
    }
   ],
   "source": [
    "y_pred = text_clf_lsvc2.predict(X_test)\n",
    "\n",
    "print(metrics.confusion_matrix(y_test,y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "3ecc3fba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         neg       0.83      0.85      0.84       188\n",
      "         pos       0.86      0.83      0.85       200\n",
      "\n",
      "    accuracy                           0.84       388\n",
      "   macro avg       0.84      0.84      0.84       388\n",
      "weighted avg       0.84      0.84      0.84       388\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(metrics.classification_report(y_test,y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "b1d6da40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8427835051546392\n"
     ]
    }
   ],
   "source": [
    "# Our score didn't change that much. Keep in mind that 2000 movie reviews is a relatively small dataset. \n",
    "# The real gain from stripping stopwords is improved processing speed.\n",
    "# Depending on the size of the corpus, it might save hours.\n",
    "\n",
    "print(metrics.accuracy_score(y_test,y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "dbd09daf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['neg']\n"
     ]
    }
   ],
   "source": [
    "# Here we will run our model on a new review\n",
    "# Be sure to put \"myreview\" inside square brackets\n",
    "# The review is considered negative by our model\n",
    "# So all three models we created predicted that our review was negative. That's good for consistency and accuracy\n",
    "\n",
    "print(text_clf_lsvc2.predict([myreview]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d71c1ac1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
