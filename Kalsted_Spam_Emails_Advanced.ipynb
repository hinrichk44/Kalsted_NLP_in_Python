{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d33e075e",
   "metadata": {},
   "source": [
    "# Project Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d0eeaaf",
   "metadata": {},
   "source": [
    "We will attempt to predict if a text is \"Ham\" or \"Spam\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27819487",
   "metadata": {},
   "source": [
    "# Feature Extraction From Text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fe38821",
   "metadata": {},
   "source": [
    "1. Most classic machine learning algorithms cannot take in raw text\n",
    "\n",
    "\n",
    "2. Instead we need to perform feature extraction from the raw text in order to pass numerical features to the ML algorithm\n",
    "\n",
    "\n",
    "3. For example, we could count the occurence of each word to map text to a number\n",
    "\n",
    "\n",
    "4. Count Vectorization: Count the occurences of all of the unique words. Each unique word will be treated as a feature. \n",
    "    \n",
    "    \n",
    "5. Then it will count each time the word shows up in the data via a matrix. This matrix will be \"sparse\"\n",
    "\n",
    "\n",
    "6. TfidfVectorizer: Alternative to CountVectorizer. It also creates a sparse matrix.\n",
    "    \n",
    "    \n",
    "7. However, instead of filling the matrix with token counts, it calcuates term frequency inverse document frequency (TF-IDF) value for each word.\n",
    "\n",
    "\n",
    "8. Term frequency is the raw count of a term in a document (the number of times that term occurs in the document)\n",
    "\n",
    "\n",
    "9. Because the term \"the\" is so common, TF will tend to incorrectly emphaize documents that use \"the\" more frequently. We need to find a way to give more weight to meaningful terms.\n",
    "\n",
    "\n",
    "10. An inverse document frequency factor is incorporated, which diminishes the weight of terms that occur very frequently and increases the weight of terms that occur rarely\n",
    "\n",
    "\n",
    "11. TF-IDF is the logarithmically scaled inverse fraction of the documents that contain the word\n",
    "\n",
    "\n",
    "12. This is obtained by dividing the total number of documents by the number of documents containing the term, and then taking the logarithm of that quotient. \n",
    "\n",
    "\n",
    "13. TD-IDF allows us to understand the context of words across an entire corpus of documents instead of just its relative importance in a single document"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bfdd0af",
   "metadata": {},
   "source": [
    "# Importing Basic Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fabfbb32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# These are the libraries I typically use in my analysis so I find it easier to import them all at once\n",
    "# If I need more libraries I will import them as needed\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "plt.style.use('fivethirtyeight')\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35eb5f33",
   "metadata": {},
   "source": [
    "# Importing the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "37b5ff04",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Our dataset is smsspamcollection.tsv, where the tsv stands for \"tab separated variables\"\n",
    "# Hence in order to import the file correctly we need to add delimiter = \"\\t\"\n",
    "# We will name the dataframe \"emails\"\n",
    "\n",
    "emails =  pd.read_csv('smsspamcollection.tsv', delimiter = '\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c85dc52a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>message</th>\n",
       "      <th>length</th>\n",
       "      <th>punct</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
       "      <td>111</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "      <td>29</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "      <td>155</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "      <td>49</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "      <td>61</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  label                                            message  length  punct\n",
       "0   ham  Go until jurong point, crazy.. Available only ...     111      9\n",
       "1   ham                      Ok lar... Joking wif u oni...      29      6\n",
       "2  spam  Free entry in 2 a wkly comp to win FA Cup fina...     155      6\n",
       "3   ham  U dun say so early hor... U c already then say...      49      6\n",
       "4   ham  Nah I don't think he goes to usf, he lives aro...      61      2"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Here is a brief look at the dataset\n",
    "# We have the dependent variable \"label\" with values ham and spam\n",
    "# We then have the actual email message, the length of the message, and the amount of punctuation in the message\n",
    "\n",
    "emails.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c9ca2fd7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5572, 4)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# There are 5572 total emails in our dataset\n",
    "\n",
    "emails.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bef9ea46",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "label      0\n",
       "message    0\n",
       "length     0\n",
       "punct      0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# There are no missing values in this dataset. Certainly makes things easier\n",
    "\n",
    "emails.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e53eeea7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ham     4825\n",
       "spam     747\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Looks like we have 4825 \"ham\" labels and 747 \"spam\" labels\n",
    "# This is definitely an unbalanced dataset and we will have to be careful about using accuracy as our metric\n",
    "\n",
    "emails['label'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0fa5ecb",
   "metadata": {},
   "source": [
    "# Creating Our Classification Model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "46455a12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# In a similar exercise we used: X = emails[['length','punct']]\n",
    "# Now we are just focused on the message column\n",
    "X = emails['message']  \n",
    "\n",
    "# y is our \"label\" data\n",
    "y = emails['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c164edaa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Data Shape: (4457,)\n",
      "Testing Data Shape:  (1115,)\n"
     ]
    }
   ],
   "source": [
    "# Here we will split our data into a training and test set\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "print('Training Data Shape:', X_train.shape)\n",
    "print('Testing Data Shape: ', X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7f8a5bee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4457, 7702)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Here we are going to perform feature extraction on our message column\n",
    "# CountVectorizer has text pre-processing, tokenizing, and the ability to filter out stop words\n",
    "# CV builds a dictionary of features and transforms documents to feature vectors\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "# We want to pass X into the instance of the CountVectorizer class\n",
    "count_vect = CountVectorizer()\n",
    "\n",
    "# Here we are fitting the vectorizer to the training data\n",
    "# This builds a vocabulary, count the number of words, etc\n",
    "# We then transform the text message to a vector\n",
    "X_train_counts = count_vect.fit_transform(X_train)\n",
    "# X_train_counts is a HUGE sparse matrix. Jupyter won't allow us to see the whole thing\n",
    "# There are 4457 messages in our training set, with 7702 unique words that are in those messages\n",
    "X_train_counts.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1c3f4cc",
   "metadata": {},
   "source": [
    "While counting words is helpful, longer documents will have higher average count values than shorter documents, even though they might talk about the same topics.\n",
    "\n",
    "To avoid this we can simply divide the number of occurrences of each word in a document by the total number of words in the document: these new features are called tf for Term Frequencies.\n",
    "\n",
    "Another refinement on top of tf is to downscale weights for words that occur in many documents in the corpus and are therefore less informative than those that occur only in a smaller portion of the corpus.\n",
    "\n",
    "This downscaling is called tf–idf for “Term Frequency times Inverse Document Frequency”.\n",
    "\n",
    "Both tf and tf–idf can be computed as follows using TfidfTransformer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "37ea9503",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4457, 7702)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "tfidf_transformer = TfidfTransformer()\n",
    "\n",
    "# So you see we are passing in our X_train_counts through here, NOT the original X_train\n",
    "X_train_tfidf = tfidf_transformer.fit_transform(X_train_counts)\n",
    "\n",
    "# The shape is still the same, but the values are no longer just counts\n",
    "# This is because we multiplied those values by the TF-IDF\n",
    "X_train_tfidf.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "66862672",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4457, 7702)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# In the future, we can combine the CountVectorizer and TfidTransformer steps into one using TfidVectorizer:\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "vectorizer = TfidfVectorizer()\n",
    "\n",
    "# Remember to use the original X_train set\n",
    "X_train_tfidf = vectorizer.fit_transform(X_train) \n",
    "\n",
    "# Once again, same shape. But everything was done in one step now. \n",
    "X_train_tfidf.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7130a2cc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LinearSVC()"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Here we will build a Support Vector Machine classifier.\n",
    "# The Linear SVC handles sparse input better, and scales well to large numbers of samples\n",
    "\n",
    "from sklearn.svm import LinearSVC\n",
    "clf = LinearSVC()\n",
    "clf.fit(X_train_tfidf,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "227431f0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('tfidf', TfidfVectorizer()), ('clf', LinearSVC())])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Remember that only our training set has been vectorized into a full vocabulary. \n",
    "# In order to perform an analysis on our test set we'll have to submit it to the same procedures. \n",
    "# Fortunately scikit-learn offers a Pipeline class that behaves like a compound classifier.\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# Pipeline object takes a list of tuples\n",
    "# Each tuple will have a string name that you decide, then you call in what you want to do\n",
    "# Basically you can run the model on the training set in one step\n",
    "# Pipline extra useful when you have stop words, lemmatization, etc\n",
    "text_clf = Pipeline([('tfidf', TfidfVectorizer()),\n",
    "                     ('clf', LinearSVC())])\n",
    "\n",
    "# Feed the training data through the pipeline\n",
    "text_clf.fit(X_train, y_train)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "bd2adb21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating predictions\n",
    "\n",
    "y_pred = text_clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0ddc9b9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[964   2]\n",
      " [  7 142]]\n"
     ]
    }
   ],
   "source": [
    "# Report the confusion matrix\n",
    "from sklearn import metrics\n",
    "print(metrics.confusion_matrix(y_test,y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3c3bb488",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         ham       0.99      1.00      1.00       966\n",
      "        spam       0.99      0.95      0.97       149\n",
      "\n",
      "    accuracy                           0.99      1115\n",
      "   macro avg       0.99      0.98      0.98      1115\n",
      "weighted avg       0.99      0.99      0.99      1115\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Print a classification report\n",
    "print(metrics.classification_report(y_test,y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "78a7ec5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9919282511210762\n"
     ]
    }
   ],
   "source": [
    "# Print the overall accuracy\n",
    "print(metrics.accuracy_score(y_test,y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7f9530b9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['ham'], dtype=object)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Here we are going to use our classifier to predict whether a new message is spam or ham\n",
    "# Our classifier predicts that it is \"ham\" or a legitimate message\n",
    "\n",
    "text_clf.predict([\"Hi how are you doing today?\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "364e3edf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['spam'], dtype=object)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Here we are going to use our classifier to predict whether a new message is spam or ham\n",
    "# Our classifier predicts that it is a spam message\n",
    "\n",
    "text_clf.predict([\"Congratulations! You've been selected as a winner. TEXT WON to 44255\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0166d24e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
